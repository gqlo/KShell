#!/usr/bin/env python3

import os
import re
import csv
import json
import glob
import sys
import argparse
import yaml
import subprocess

# Python scripts process promethus JSON raw metrics

def sys_exit(str):
    print(f"{str}")
    sys.exit(1)

def is_dir(dir_path):
    if not os.path.isdir(dir_path):
        print(f"The directory '{dir_path}' does not exits")
        return False
    return True

def file_is_readable(file_path):
    if not os.access(file_path, os.R_OK):
        print(f"The file at '{file_path}' is not readable")
        return False
    return True

def obj_exist(obj):
    return obj is not None

def check_meta_data(json_obj):
    status = json_obj.get("status")
    if obj_exist(status) and status == "success":
        print("Promethus query status returned success")
    else:
        sys_exit("Promethus query status returned non-success or status value is not present")
    data = json_obj.get("data")
    res_type = data.get("resultType")
    res = data.get("result")
    if obj_exist(data) and obj_exist(res_type) and obj_exist(res) and len(res) >= 1:
        print(f"{len(res)} entries of data found, resultType: {res_type}, ")
    else:
        sys_exit("No data found, please check your json file")

# read the path of json files with given directory
def read_json_files(dir_path):
    if is_dir(dir_path):
        return glob.glob(f"{dir_path}/*.json")

def read_json_file(file_path):
    try:
        with open(file_path, "r") as file:
            return json.load(file)
    except FileNotFoundError:
        print(f"Error: file '{file_path}' does not exists")
    except IOError:
        print(f"Error: could not open file '{file_path}'")

# row expects an array
def csv_write_row(path, row):
    try:
        with open(path, mode='a', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(row)
    except Exception as e:
        print(f"Error: '{e}' occured while writing data into '{path}'")

"""
Assume promethus metric maintains a consistent structure as follows:
{ "status": "success",
  "data": {"resultType": "xx",
           "result": [
                      {"metric": {meta_data}, "values": [[time_stamp,value]...]},
                      {"metric": {meta_data}, "values": [[time_stamp,value]...]}
                     ]
          }
}
"""
# process json files from the entire directory
def json_dir_to_csv(dir_path):
    file_paths = read_json_files(dir_path)
    if len(file_paths) == 0:
        print("No json file is found")
        return
    for file_path in file_paths:
        json_obj = read_json_file(file_path)
        check_meta_data(json_obj)
        result = json_obj['data']["result"]
        headers = []
        values  = []
        for item in result:
            headers.append(item['metric']['pod'] + "-" + re.sub(r'^.*/([^/]+)\.\w+$', r'\1', file_path))
            values.append([value[1] for value in item['values']])
        # use zip to transpose lists to do column by column write
        csv_file_path=re.sub(r"\.json$", ".csv", file_path)
        csv_write_row(csv_file_path, headers)
        for row in zip(*values):
            csv_write_row(csv_file_path, row)
        print(f"Finished writting data into csv file at location: '{csv_file_path}'")

# process a single json file
def json_file_to_csv(file_path, scale):
    if file_is_readable(file_path):
        json_obj = read_json_file(file_path)
    check_meta_data(json_obj)
    result = json_obj['data']["result"]
    csv_file_path=re.sub(r"\.json$", ".csv", file_path)
    csv_write_row(csv_file_path, ["Container", "Max", "Avg"])
    for item in result:
        values = []
        metric_names = list(item['metric'].keys())
        if len(metric_names) < 1:
            sys_exit("no metric names found, please check the json file")
        container_name = '-'.join([ item['metric'][key] for key in metric_names])
        values = [float(value[1])/scale for value in item['values']]
        non_zero_values = [value for value in values if value != float(0)]
        print(container_name, len(non_zero_values), len(values))
        max_val = 0.0 if len(non_zero_values) == 0 else max(non_zero_values)
        mean_val = 0.0 if len(non_zero_values) == 0 else sum(non_zero_values)/float(len(non_zero_values))
        row = [container_name, max_val, mean_val]
        csv_write_row(csv_file_path, row)

def is_int(num):
    try:
        int(num)
        return True
    except ValueError:
        print("Error: expecting the time stamp to be an integer")
        return False

def check_time_stamp(stamp):
    if len(stamp) != 4:
        sys_exit("expecting 4 parts in a stamp, <namespace> <job type> <start time> <end time>")
    if is_int(stamp[2]) and is_int(stamp[3]) and (int(stamp[3]) - int(stamp[2])) < 0:
        sys_exit("end time stamp is before start time stamp")

def read_time_stamps(file_path):
    jobs = {}
    if file_is_readable(file_path):
        with open(file_path, 'r') as file_obj:
            for line in file_obj:
                if line.strip():
                    stamp = line.strip().split()
                    check_time_stamp(stamp)
                jobs["_".join(stamp[:2])]="_".join(stamp[2:])
    return jobs

def read_yaml(file_path):
    if file_is_readable(file_path):
         with open(file_path, 'r') as file:
            return yaml.safe_load(file)

def parse_metric_profile(file_path):
    if file_is_readable(file_path):
        return read_yaml(file_path)

# jobs is a dict uses namespace_job_type as the key, start_end timestamp as the value
def extract_prom_json_data(metric_profile, jobs):
    for job in jobs.keys():
        start = jobs[job].split("_")[0]
        end = jobs[job].split("_")[1]
        for metric_name in metric_profile.keys():
            QUERY_EXPRESSION=metric_profile[metric_name]
            urlencode_cmd = ["urlencode", QUERY_EXPRESSION]
            encoded_result = subprocess.run(urlencode_cmd, capture_output=True, text="True")
            encoded_expr = encoded_result.stdout.replace('\n', '').replace('\r', '')
            curl_cmd = (
            f"oc exec -n openshift-monitoring -c prometheus prometheus-k8s-0 -- "
            f"curl -s 'http://localhost:9090/api/v1/query_range?"
            f"query={encoded_expr}&start={start}&end={end}&step=30s' | "
            f"jq > {job}_{metric_name}.json"
            )
            query_result = subprocess.run(curl_cmd, capture_output=True, shell=True)
            print(query_result.stdout)
            print(query_result.stderr)
            print("status=", query_result.returncode)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="command line options for promethus data processing.")
    parser.add_argument('-s', '--scale', type=int, default=1, help="the scale factor number to be divided")
    parser.add_argument('-j', '--file', type=str, help="json file path, extract data points as csv format")
    parser.add_argument('-t', '--timestamp', type=str, help="timestamp file path")
    parser.add_argument('-p', '--profile', type=str, help="promethus metric profile path")

    args = parser.parse_args()
    if bool(args.timestamp) != bool(args.profile):
        parser.error("both --timestamp and --profile file path must be passed together")

    if args.file:
        json_file_to_csv(args.file, args.scale)
    elif args.profile or args.timestamp:
        jobs = read_time_stamps(args.timestamp)
        metric_profile = read_yaml(args.profile)
        extract_prom_json_data(metric_profile, jobs)
