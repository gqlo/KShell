#!/bin/bash

# helper script to launch kubevirt guest clusters

# set scrict mode
set -Eeuo pipefail
# some global constants
cluster_name=$(echo $RANDOM | md5sum | head -c 10)      # random string as default cluster name
prog="${0##*/}"                                         # program name
usage="Usage: $prog [ <cluster_name> [ <test_id> ] ]"   # usage string
conf_file="$HOME/hypershift/config/.$prog.conf"         # config file name
start_time=$(date +%s)                                  # script start time
time="$(date -d "@$start_time" '+%F_%T')"               # time string
div="$(printf '=%.s' {1..40})"                          # divider line string
param_string=""                                         # parameter string
PARALLEL=false                                          # determine if it's a parallel cluster creation
OC=$(command -v oc)                                     # check if oc cli exists
HS=$(command -v hypershift)                             # check if hypershift cli exits
URLENCODE=$(command -v urlencode)                       # check if urlencode exits
declare -a timing_phase=()                              # array to store timings of each phase
declare -A PROM_METRIC=()                               # store metirc name and query
declare -A JOBS=()                                      # store job names start and end time stamps
declare -i CLUSTER_NUM=1                                # number of hosted cluster to be created

#-- functions ------------------------------------------------------------------
help_msg() {
cat <<EOF
h-bench is a tool allow you to launch guest clusters within KubeVirt VMs at scale.

Usage: [Options] [name]
    Help:
        -h              Print basic help information

    Options:
        -f  jobfile     Job file describe configurations of the guest cluster. Some examples
                        are included in the examples/ directory.
        -c  name        Create guest cluster kubeconfig file at the config directory.
        -d  name        Destory the guest cluster by name. Guest kubeconfig will be deleted if exits.
        -p  profile     Extract promethus raw metrics based on provided metric profile and save it as
                        Json format. Examples are included in metric_profile/ dirctory.
        -l  bool        Flag style option, indicates if create tenant cluster in parellel or sequentially,
                        deafult to: $PARALLEL.
        -n  int         Number of tenant clusters to create, default to $CLUSTER_NUM.
EOF
}

# die fn takes an optional error message to display
# if missing, the error message defaults to the usage string
die() {
    printf '%s\n' "${@:-$usage}" >&2
    exit 1
}

# none of the functions below use global vars
# so all input is passed as arguments
process_option() {
    local line="$1"
    param_string+="--$(echo "$line" | sed 's/=.*//') $(echo "$line" | sed 's/[^=]*=//') "
}

process_job_file() {
    local jobfile="$1"
    if [[ ! -f $jobfile || ! -r $jobfile ]]; then
        die "Job file does not exists or cannot be read"
    fi
    while IFS= read -r line; do
        line=$(echo "$line" | sed 's/ //g')
        if [[ $line =~ ^name= ]]; then
            cluster_name=${line##*=}
        fi
        [[ -z "$line" ]] && continue
        process_option "$line"
    done < "$jobfile"
    create_multi_clusters
}

create_cluster() {
    local cluster_param="$1"
    local cluster="$2"
    local start_time=$(date +%s; date +'%Y-%m-%d %T')
    {
        printf '%s\n' "$div $cluster $div"
        SECONDS=0
        hypershift create cluster kubevirt $(echo "$cluster_param"| sed 's/"//g')
        create_secs=$SECONDS
        echo "create_secs=$create_secs"
    } &> >(tee -a "$control_log")
    log_creation_phase $cluster
    timing_phase=(${timing_phase[@]:0:1} $create_secs ${timing_phase[@]:1})
    write_provision_time_to_csv
    echo "Cluster=$cluster namespace=clusters-$cluster Start time: "$start_time" End time: $(date +%s) $(date +'%Y-%m-%d %T')" | tee -a "$time_stmp"
}

# regx matches zero or more spaces followed by "--name"
# one or more spaces and one more more characters except space
# append the first capture group with "-$i"
# use double quotes to allow variable expansion.
create_multi_clusters() {
    local n=1
    local i=1
    if [[ ! $param_string =~ "--name" ]]; then
        param_string+="--name $cluster_name"
    fi
    echo "CLUSTER_NUM $CLUSTER_NUM"
    if [[ $PARALLEL == true ]]; then
       for ((n=1; n<=CLUSTER_NUM; n++)); do
        {
    	    local cluster_param=$(echo $param_string | sed  -E "s/( *)--name[[:space:]]+([^ ]+)/\1&-$n/")
            local cluster_name=$(echo $cluster_param | sed -E "s/.*--name ([^ ]+).*/\1/")
	    echo "Parallel: $cluster_param"
	    create_cluster "$cluster_param" "$cluster_name"
        } &
       done
    else
       for ((i=1; i<=CLUSTER_NUM; i++)); do
    	    local cluster_param=$(echo $param_string | sed  -E "s/( *)--name[[:space:]]+([^ ]+)/\1&-$i/")
            local cluster_name=$(echo $cluster_param | sed -E "s/.*--name ([^ ]+).*/\1/")
            echo "seq creation: $cluster_param $cluster_name"
	    create_cluster "$cluster_param" "$cluster_name"
            sleep 30
            kube_burner_run "$cluster_name"
       done
    fi
}

get_domain() {
    oc get ingresscontroller -n openshift-ingress-operator \
        default -o jsonpath='{.status.domain}'
}

destory_cluster() {
    local name="$1"
    if [[ -z "$HS" ]]; then
        die "HyperShift cli is not installed"
    fi
    if [[ "$1" == "--all" ]]; then
        for cluster in $(oc get hc -A | awk  'NR>1 {print $2}'); do
        {
            hypershift destroy cluster kubevirt --name "$cluster"
            [[ -f "$dir_conf/$cluster" ]] && sudo rm "$dir_conf/$cluster"
        } &
        done
    else
        hypershift destroy cluster kubevirt --name "$name"
        [[ -f "$$dir_conf/$name" ]] && sudo rm "$dir_conf/$name"
    fi
}

wait_provision() {
    local timeout="$1"
    local cluster="$2"
    let timeout+=$(date +%s)
    while sleep 5; do
        oc get vms -n "clusters-$cluster" | grep -qF "$cluster" && break
        (($(date +%s)>timeout)) \
           && die "Timeout while waiting for provisioning of $cluster"
    done
}

wait_ready() {
    local timeout="$1"
    local cluster="$2"
    oc wait --for "condition=Ready" --namespace "clusters-$cluster" \
        vm --all --timeout "${timeout}s"
    # need to figure out a way to know if it timed out or not
}

wait_available() {
    local kube_conf="$1"
    local timeout="$2"
    oc wait --for "condition=Available=True" --kubeconfig "$kube_conf" \
        co --all --timeout "${timeout}s"
    # need to figure out a way to know if it timed out or not
}

create_kubeconf() {
    local name="$1"
    hypershift create kubeconfig --name="$name" > "$dir_conf/$name"
}

get_nodeport() {
    local kube_conf="$1"
    oc --kubeconfig "$kube_conf" get services \
    -n openshift-ingress router-nodeport-default \
    -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}'
}

wait_nodeport() {
    local cluster="$1"
    local kube_conf="$2"
    local timeout="$3"
    let timeout+=$(date +%s)
    while sleep 2; do
        local port="$(get_nodeport "$kube_conf")"
        [[ -n "$port" ]] && break
        (($(date +%s)>timeout)) \
            && die "Timeout while waiting for nodeport on $cluster"
    done
    printf '%s\n' "$port"
}

write_provision_time_to_csv() {
    local i=0
    local header=("name" "create_secs" "provision_secs" "worker_secs" "config_secs" "nodeport_secs" "operator_secs" "total_secs")
    # if csv file doesn't exist, create it with a header line
    if [[ ! -e "$timing_csv" ]]; then
        sed 's/ /","/g;s/.*/"&"/' <<<"${header[*]}" > "$timing_csv"
    fi

    # write out the field values, first one quoted, the rest not quoted
    {   for (( i=0; i<${#timing_phase[@]}; i++ )); do
            if ((i==0)); then
                printf '"%s"' "${timing_phase[$i]}"
            else
                printf ',%s'  "${timing_phase[$i]}"
            fi
        done
        echo
    } >> "$timing_csv"
}

kube_burner_run() {
    local cluster_name="$1"
    local WORKLOAD="cluster-density"
    local start_time=$(date +%s)
    local target_dir="$(pwd)"/"e2e-benchmarking/workloads/kube-burner"
    if [[ ! -d "e2e-benchmarking" ]]; then
        git clone https://github.com/cloud-bulldozer/e2e-benchmarking.git
    fi
    if [[ ! -e "$dir_conf/$cluster_name" ]]; then
        create_kubeconf "$cluster_name"
    fi
    if [[ ! -f "$dir_conf/$cluster_name" || ! -r "$dir_conf/$cluster_name" ]]; then
        die "Tenant cluster kube-config file does not exist or it can not be read"
    fi
    {
        (export KUBECONFIG="$dir_conf/$cluster_name"
        echo "Running kube-burner $WORKLOAD workload against the following nodes"
        oc get nodes | head
        cd "$target_dir" && JOB_ITERATIONS=10 INDEXING=false WORKLOAD="$WORKLOAD" ./run.sh)
        local end_time=$(date +%s)
        echo "Cluster=$cluster_name namespace=clusters-$cluster_name Start time: $start_time $(date -d "@$start_time" +"%Y-%m-%d %T") End time: $end_time  $(date -d "@$end_time" +"%Y-%m-%d %T")"
    }   &> >(tee -a "$burner_run_log")
    echo "clusters-$cluster_name $WORKLOAD $start_time $end_time" | tee -a "$burner_timestamp"
}

# inital version of metric profile parsing, not yet robust.
parse_metric_profile() {
    local metric_profile="$1"
    if [[ ! -f $metric_profile || ! -r $metric_profile ]]; then
        die "metric profile does not exits or can not be read"
    fi

    if [[ -z $URLENCODE ]]; then
        die "urlencode cli does not exist"
    fi

    while IFS= read -r m_name; do
        read -r m_query
            local m_name=$(echo $m_name | sed 's/://')
            local m_query=$(echo $m_query | sed 's/^ *- //g')
            # encoded_query=$(urlencode $m_query)
            PROM_METRIC[$m_name]=$m_query
    done < "$metric_profile"
}

find_max() {
    local arr="$1"
    local max=0
    for num in "${arr[@]}"; do
        if (($num > $max)); then
            max=$num
        fi
    done
    echo $max
}

# read namespace kube-burner workload name from a text file
read_timestamps() {
    local line=""
    while IFS= read -r line; do
        read -a stamps <<< "$line"
        local key="${stamps[0]}":"${stamps[1]}"
        if [[ "${!JOBS[@]}" =~ "$key" ]]; then
            die "Seems like you have duplicated workdload runs"
        fi
        JOBS[$key]="${stamps[2]}"
        echo "$key workload run time spent: $(( ${stamps[3]} - ${stamps[2]} ))"
    done < "$burner_timestamp"
}

# Extract promethus raw metrics with given profile and timestamps.
# Use scrape interval as the steps to get all data points.
prom_extract() {
    local metric_profile="$1"
    parse_metric_profile "$metric_profile"
    read_timestamps
    for job in "${!JOBS[@]}"; do
        start="${JOBS[$job]}"
        end=$(( ${JOBS[$job]} + 7200 ))
        namespace="${job%%:*}"
        kube_workload="${job##*:}"
        for metric_name in "${!PROM_METRIC[@]}"; do
            QUERY_EXPRESSION=${PROM_METRIC[$metric_name]}
            if grep -q "filter" <<< "$QUERY_EXPRESSION"; then
                QUERY_EXPRESSION=${QUERY_EXPRESSION/\$filter/"namespace=\"$namespace\""}
            fi
            QUERY_EXPRESSION=$(urlencode $QUERY_EXPRESSION)
            oc exec -n openshift-monitoring -c prometheus prometheus-k8s-0 -- \
            curl -s "http://localhost:9090/api/v1/query_range?query=${QUERY_EXPRESSION}&start=${start}&end=${end}&step=30s" \
            | jq > "$dir_out/$kube_workload-$namespace-$metric_name".json
            echo finished extracting metrics: "$dir_out/$kube_workload-$namespace-$metric_name".json
        done
    done
}

create_ingress_service() {
    local cluster="$1"
    local namespace="$2"
    local nodeport="$3"
    oc apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  labels:
    app: $cluster
  name: apps-ingress
  namespace: $namespace
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: https-443
    port: 443
    protocol: TCP
    targetPort: $nodeport
  selector:
    kubevirt.io: virt-launcher
  sessionAffinity: None
  type: ClusterIP
EOF
}

create_ingress_route() {
    local cluster="$1"
    local namespace="$2"
    local cluster_domain="$3"
    oc apply -f - <<EOF
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: ${cluster}-443
  namespace: $namespace
spec:
  host: data.apps.$cluster_domain
  wildcardPolicy: Subdomain
  tls:
    termination: passthrough
  port:
    targetPort: https-443
  to:
    kind: Service
    name: apps-ingress
    weight: 100
EOF
}

log_creation_phase() {
    local cluster="$1"
    local start_time=$(date +%s)
    {   printf '%s\n' "$div $cluster $div" "Waiting to provision $cluster"
        SECONDS=0
        wait_provision "3600" "$cluster"
        provision_secs=$SECONDS
        echo "provision_secs=$provision_secs"
        SECONDS=0
        wait_ready "3600" "$cluster" # timeout 1 hour
        worker_secs=$SECONDS
        echo "worker_secs=$worker_secs"
    }   &> >(tee -a "$provision_log")

    {   printf '%s\n' "$div $cluster $div"
        SECONDS=0
        create_kubeconf "$cluster"
        config_secs=$SECONDS
        echo "config_secs=$config_secs"
        SECONDS=0
        nodeport="$(wait_nodeport "$cluster" "$dir_conf/$cluster" "30")"  # timeout 30 secs
        nodeport_secs=$SECONDS
        echo "nodeport_secs=$nodeport_secs"
        printf '%s\n' "$cluster https nodeport: $nodeport"
    }   &> >(tee -a "$ingress_log")

    {   printf '%s\n' "$div $cluster $div"
        SECONDS=0
        wait_available "$dir_conf/$cluster" "3600"   # timeout 1 hour
        operator_secs=$SECONDS
        echo "operator_secs=$operator_secs"
        total_secs="$(($(date +%s)-start_time))"
        echo "total_secs=$total_secs"
    }   &> >(tee -a "$deploy_log")
    timing_phase=("$cluster" "$provision_secs" "$worker_secs" "$config_secs" "$nodeport_secs" "$operator_secs" "$total_secs")
}

#-- configuration --------------------------------------------------------------
# create a config file if none exists
if [[ ! -e "$conf_file" ]]; then
    cat <<'END' > "$conf_file"

#edit this config file with care (normal bash syntax)
#need to find a way to check if this config file is updated

dir_out="$HOME/hypershift/output"                               # directory containing output files
dir_conf="$HOME/hypershift/config"                              # config directory
dir_log="$HOME/hypershift/log"                                  # directory containing log files
pull_secret="$dir_conf/pull-secret"                             # file containing the pull secret key

# list of log and csv files
control_log="$dir_log/control.log"                              # control plane output
provision_log="$dir_log/provision.log"                          # provisioning phase output
ingress_log="$dir_log/ingress.log"                              # ingress phase output
deploy_log="$dir_log/deployment.log"                            # deployment phase output
timing_csv="$dir_log/timing-phase.csv"                          # timings in CSV format
time_stmp="$dir_log/stamp.log"                                  # time stamps
burner_timestamp="$dir_out/burner-run.log"                      # kube-burner workload run time stamp ready to be processed
burner_run_log="$dir_log/kube-burner-run.log"                   # kube-burner workload run log

END
    # report this to the user so they know where to find it
    echo >&2 "New config file created at $conf_file"
fi

# source the config file
source "$conf_file"

#-- Top Level -----------------------------------------------------------------
while getopts "hf:ln:c:d:p:" opt; do
    case "$opt" in
        h) help_msg                                 ;;
        f) process_job_file "$OPTARG"               ;;
        l) PARALLEL=true                            ;;
        n) CLUSTER_NUM="$OPTARG"                    ;;
        c) create_kubeconf  "$OPTARG"               ;;
        d) destory_cluster  "$OPTARG"               ;;
        p) prom_extract     "$OPTARG"               ;;
        *) die "invalid options"                    ;;
    esac
done

if [[ -z "$HS" ]]; then
    die "hypershift cli is not installed"
fi
